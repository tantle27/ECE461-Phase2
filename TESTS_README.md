# ğŸ§ª Test Suite Documentation

**ECE461 Phase 2 - Trustworthy Model Registry**  
**Total Tests: 190** | **Coverage: 42%** | **Status: âœ… All Passing**

---

## ğŸ“Š **Test Suite Overview**

This repository contains a comprehensive test suite covering both **Phase 1** (CLI-based model evaluation) and **Phase 2** (REST API model registry) functionality.

```
â”œâ”€â”€ Phase 1 Tests: 137 tests (Original CLI system)
â”‚   â”œâ”€â”€ API Layer: 46 tests (External integrations)
â”‚   â”œâ”€â”€ Metrics Layer: 91 tests (Algorithm precision)
â”‚   â””â”€â”€ Application Layer: 0 tests (CLI removed)
â””â”€â”€ Phase 2 Tests: 53 tests (New REST API scaffolding)
    â”œâ”€â”€ Backend Services: 16 tests
    â”œâ”€â”€ New Metrics: 15 tests
    â””â”€â”€ API Endpoints: 22 tests
```

---

## ğŸ¯ **Quick Start**

```bash
# Run all tests
python -m pytest

# Run with coverage (includes both src/ and app/ directories)
python -m pytest --cov=src --cov=app

# Run specific test categories
python -m pytest -m "unit"          # Unit tests only
python -m pytest -m "integration"   # Integration tests only
python -m pytest -m "api"           # API tests only
python -m pytest -m "backend"       # Backend tests only
python -m pytest -m "slow"          # Performance tests only

# Run Phase 1 vs Phase 2 tests
python -m pytest tests/api/ tests/metrics/ tests/test_main.py  # Phase 1
python -m pytest tests/test_*.py --ignore=tests/test_main.py   # Phase 2
```

---

## ğŸ—ï¸ **Phase 1 Tests (144 tests) - CLI Model Evaluation**

### ğŸŒ **API Layer Tests (46 tests)**

#### `test_gen_ai_client.py` - **21 tests** ğŸ¤–
**Purpose**: Tests AI integration with Purdue's GenAI API for performance analysis

- **Basic Communication (4 tests)**
  - âœ… Successful API calls and responses
  - âš ï¸ Error handling (400, 401, authentication failures)
  - ğŸ”§ Custom model selection and API configuration

- **Performance Claims Analysis (8 tests)**
  - ğŸ“Š Extracts benchmark metrics from README files
  - ğŸ“ Handles markdown code blocks and JSON parsing
  - ğŸ”„ Fallback strategies for malformed responses
  - ğŸ›¡ï¸ Edge case handling (nested braces, invalid JSON)

- **README Clarity Analysis (9 tests)**
  - ğŸ“– AI-powered documentation quality scoring
  - ğŸ”¢ Numerical score extraction and validation
  - ğŸ¯ Perfect scores (1.0) and zero scores (0.0)
  - ğŸ¤ª AI misbehavior and "LLM disobedience" handling

#### `test_git_client.py` - **12 tests** ğŸ“
**Purpose**: Tests Git repository operations with REAL repositories

- **Repository Management (3 tests)**
  - ğŸ—ï¸ Creates actual Git repositories for testing
  - ğŸ§¹ Cleanup and temporary directory management
  - âŒ Error handling for invalid repository URLs

- **Commit Analysis (5 tests)**
  - ğŸ‘¥ Analyzes commit history and contributor patterns
  - ğŸ“ˆ Handles empty repos and single-author scenarios
  - ğŸ” Multi-contributor analysis and bus factor calculation

- **Code Quality Analysis (3 tests)**
  - ğŸ Python code quality metrics and linting
  - ğŸ“¦ Handles repositories without Python files
  - ğŸ›¡ï¸ Error handling for inaccessible files

- **Repository Size Analysis (2 tests)**
  - ğŸ“ Calculates repository size for hardware compatibility
  - âš¡ Performance optimization for large repositories

#### `test_git_client_coverage.py` - **7 tests** ğŸ›¡ï¸
**Purpose**: Extended edge case testing for Git operations

- **Error Conditions & Edge Cases**
  - ğŸš« Non-existent repository handling
  - ğŸ’¥ Corrupted repository recovery
  - ğŸ“„ Missing README file scenarios
  - ğŸ”’ File permission and access errors
  - ğŸ“ Empty directory and cleanup edge cases

#### `test_hugging_face_client.py` - **6 tests** ğŸ¤—
**Purpose**: Tests HuggingFace API integration for dataset metrics

- **Mathematical Normalization (3 tests)**
  - ğŸ“Š Logarithmic scaling for popularity metrics
  - 0ï¸âƒ£ Zero value handling and boundary conditions
  - ğŸ“ˆ Maximum value capping and normalization

- **Dataset Information (2 tests)**
  - â¤ï¸ Fetches dataset likes and download counts
  - ğŸ” Handles missing metadata gracefully

- **File Operations (1 test)**
  - ğŸ“¥ HuggingFace file download functionality

### ğŸ¯ **Metrics Layer Tests (91 tests)**

#### `test_license_metric.py` - **20 tests** âš–ï¸
**Purpose**: License detection and compatibility scoring

- **License Type Recognition (5 tests)**
  - âœ… Permissive licenses (MIT, Apache, BSD) â†’ Score: 1.0
  - âš ï¸ Weak copyleft (LGPL) â†’ Score: 0.5
  - ğŸ”’ Strong copyleft (GPL, AGPL) â†’ Score: 0.1
  - âŒ No license â†’ Score: 0.0

- **Text Processing & Edge Cases (8 tests)**
  - ğŸ” Case-insensitive license detection
  - ğŸ‡¬ğŸ‡§ British spelling ("Licence" vs "License")
  - ğŸ“ Multiple license sections and formats
  - ğŸ“„ Missing README or empty license sections

- **Integration & Validation (7 tests)**
  - ğŸ”— End-to-end Git client integration
  - âœ”ï¸ Input validation and type checking
  - ğŸ§ª Real-world license format testing

#### `test_dataset_code_metric.py` - **17 tests** ğŸ—‚ï¸
**Purpose**: ML repository completeness analysis

- **Core ML Assessment (4 tests)**
  - ğŸ† Dataset + Training Code â†’ Score: 1.0
  - ğŸ“Š Only dataset information â†’ Score: 0.5
  - ğŸ’» Only training code â†’ Score: 0.5
  - âŒ Neither present â†’ Score: 0.0

- **File Pattern Recognition (6 tests)**
  - ğŸƒ Training scripts (train.py, finetune.py, etc.)
  - ğŸŒ Dataset URLs (Kaggle, HuggingFace, academic)
  - ğŸ“ Data files (.csv, .json, .parquet)
  - ğŸ““ Jupyter notebook analysis (.ipynb)
  - ğŸ“‚ Data directory detection (/data/, /datasets/)

- **Configuration & Metadata (3 tests)**
  - âš™ï¸ config.json dataset references
  - ğŸ·ï¸ Repository type classification (model/dataset/training)
  - ğŸ”— Enhanced dataset source detection

- **Edge Cases (4 tests)**
  - ğŸ“­ Empty repository handling
  - ğŸ“„ Missing README scenarios
  - ğŸ›¡ï¸ Malformed configuration handling

#### `test_ramp_up_time_metric.py` - **15 tests** ğŸ“š
**Purpose**: Learning curve and getting-started assessment

- **Scoring Components (5 tests)**
  - ğŸ† Perfect score: Documentation + Examples + Dependencies
  - âŒ Zero score: No helpful materials
  - âš–ï¸ Partial scores: Weighted combination
  - ğŸ“– Documentation-only scenarios
  - ğŸ› ï¸ Technical-only scenarios (examples + deps)

- **Algorithm Validation (3 tests)**
  - ğŸ§® Weight constants sum to 100%
  - âš–ï¸ Proper weight distribution
  - ğŸ”¢ Boundary value testing

- **Error Handling (5 tests)**
  - ğŸ“Š Missing data handling
  - ğŸ§© Incomplete information scenarios
  - ğŸ›¡ï¸ Input validation and type checking
  - ğŸš« Null input handling

- **Exception Recovery (2 tests)**
  - ğŸ¤– AI client failure recovery
  - ğŸ“ Git client failure handling

#### `test_size_metric.py` - **10 tests** ğŸ’¾
**Purpose**: Hardware compatibility assessment

- **Hardware Compatibility Tiers (4 tests)**
  - ğŸ“ Raspberry Pi compatible (< 100MB)
  - ğŸš€ Jetson Nano compatible (< 1GB)
  - ğŸ–¥ï¸ Desktop PC compatible (< 10GB)
  - â˜ï¸ AWS Server only (unlimited)

- **Edge Cases & Errors (6 tests)**
  - ğŸ“­ Empty repository handling
  - ğŸš« Null response scenarios
  - ğŸ”¢ Invalid size data
  - ğŸ§© Missing size information
  - 0ï¸âƒ£ Zero-size edge cases
  - âš ï¸ Git client error recovery

#### Other Metric Tests (45 tests)

- **`test_bus_factor_metric.py`** (7 tests) ğŸ‘¥
  - Uses Herfindahl-Hirschman Index for contributor diversity
  - Perfect distribution â†’ 0.8, Single author â†’ 0.0

- **`test_code_quality_metric.py`** (9 tests) ğŸ”§
  - Lint errors, test presence, code complexity analysis
  - Scoring based on error count and test coverage

- **`test_dataset_quality_metric.py`** (4 tests) ğŸ“ˆ
  - HuggingFace popularity metrics with logarithmic scaling
  - Missing metadata and API failure handling

- **`test_performance_claims_metric.py`** (5 tests) ğŸ†
  - AI-powered benchmark extraction from documentation
  - JSON parsing and performance metric validation

- **`test_metric.py`** (2 tests) ğŸ—ï¸
  - Abstract base class functionality
  - Input/output contract validation

- **`test_metrics_calculator.py`** (2 tests) ğŸ¼
  - End-to-end metric calculation pipeline
  - Success and failure path integration testing

### ğŸ–¥ï¸ **Application Layer Tests (7 tests)**

#### `test_main.py` - **6 tests** ğŸ¯
**Purpose**: Command-line interface and orchestration

- **URL File Processing (2 tests)**
  - ğŸ“„ Parse URL files with new format (GitHub, HuggingFace, datasets)
  - âŒ File not found error handling

- **Async Processing (2 tests)**
  - âš¡ Async entry processing with NDJSON output
  - ğŸ”— Integration with metrics calculator

- **CLI Interface (2 tests)**
  - ğŸ›ï¸ Command-line argument validation
  - ğŸ”§ Main function orchestration with file input

#### `test_parallelism.py` - **1 test** ğŸš€
**Purpose**: Performance validation

- **Concurrency Requirements**
  - âš¡ Ensures concurrent processing is 1.5x faster than sequential
  - ğŸ“Š Real performance measurement with timing
  - ğŸ¯ Validates Phase 1 performance goals

---

## ğŸ†• **Phase 2 Tests (53 tests) - REST API Scaffolding**

### ğŸ—ï¸ **Backend Services**

#### `test_backend.py` - **16 tests** ğŸ”§
**Purpose**: Core backend service testing infrastructure

- **Model Registry (3 tests)**
  - ğŸ“¦ Model upload structure validation
  - ğŸ·ï¸ Metadata validation and processing
  - ğŸ”— Scoring system integration

- **Database Operations (3 tests)**
  - ğŸ—„ï¸ Database connection mocking
  - ğŸ“ CRUD operations for models and users
  - ğŸ‘¤ User management and authentication

- **File Operations (2 tests)**
  - ğŸ“¤ File upload validation and processing
  - â˜ï¸ AWS S3 integration testing

- **Authentication & Authorization (3 tests)**
  - ğŸ” JWT token generation and validation
  - ğŸ›¡ï¸ Permission and access control
  - ğŸš¦ Rate limiting functionality

- **API Endpoints (3 tests)**
  - â¤ï¸ Health check endpoints
  - ğŸ” Model search functionality
  - ğŸ“¥ Model ingestion from HuggingFace

- **Performance (2 tests)**
  - âš¡ Concurrent upload handling
  - ğŸ“Š Database query performance

#### `test_new_metrics.py` - **15 tests** ğŸ“Š
**Purpose**: Phase 2 enhanced metrics

- **Reproducibility Metric (4 tests)**
  - âœ… Working demo code execution â†’ Score: 1.0
  - ğŸ”§ Partial working code (needs debugging) â†’ Score: 0.5
  - âŒ No demo code available â†’ Score: 0.0
  - ğŸ“– Model card code extraction and testing

- **Reviewedness Metric (4 tests)**
  - ğŸ“ˆ High PR review coverage (85%) â†’ Score: 0.85
  - ğŸ“‰ No pull requests â†’ Score: -1.0
  - ğŸš« No GitHub repository â†’ Score: -1.0
  - ğŸ† Perfect review coverage â†’ Score: 1.0

- **Treescore Metric (4 tests)**
  - ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Parent model dependency scoring
  - ğŸš« No parent models â†’ Score: 0.0
  - ğŸ‘¤ Single parent model inheritance
  - ğŸ§© Missing parent score handling

- **Integration Tests (3 tests)**
  - ğŸ”— All metrics working together
  - â±ï¸ Latency tracking for new metrics
  - ğŸ¯ Enhanced net score calculation

#### `test_api_endpoints.py` - **22 tests** ğŸŒ
**Purpose**: REST API endpoint scaffolding

- **Model Registry API (5 tests)**
  - ğŸ“¤ Model upload with metadata
  - âŒ Invalid data handling
  - ğŸ” Model retrieval by ID
  - ğŸ” Search with query parameters
  - âœï¸ Model metadata updates
  - ğŸ—‘ï¸ Model deletion

- **Package Registry API (3 tests)**
  - ğŸ“¦ Package upload functionality
  - ğŸ“Š Package metrics retrieval
  - ğŸ” Regex-based package search

- **Authentication API (5 tests)**
  - ğŸ“ User registration
  - ğŸ” Login and token management
  - âŒ Invalid credential handling
  - ğŸ”„ Token refresh functionality
  - ğŸšª User logout

- **Error Handling (4 tests)**
  - ğŸš¦ Rate limiting (429 errors)
  - ğŸ’¥ Server error handling (500 errors)
  - âœ… Input validation (422 errors)
  - ğŸ” Not found errors (404)

- **Performance Testing (5 tests)**
  - âš¡ Concurrent model uploads
  - ğŸ“Š Large search result handling
  - â±ï¸ Long-running metrics calculations
  - ğŸ¯ Load testing scenarios

---

## ğŸ“ˆ **Test Coverage Analysis**

### Current Coverage: **42%** (868 missed / 1503 total lines)

**Includes both `src/` and `app/` directories**

#### High Coverage Areas âœ…
- **Core Metrics Logic**: 55-86% (Well-tested algorithms)
- **Performance Claims**: 100% (AI integration working)
- **Size Metrics**: 100% (Hardware compatibility)
- **License Detection**: 97% (Strong foundation)

#### Medium Coverage Areas âš ï¸
- **Git Client**: 70% (Core functionality covered)
- **GenAI Client**: 71% (Main features tested)
- **Metrics Calculator**: 56% (Integration paths)

#### Zero Coverage Areas âŒ
- **Flask App Core**: 0% (app/core.py - 775 lines, 18 endpoints)
- **Flask App Setup**: 0% (app/app.py - application factory)
- **Flask Scoring**: 0% (app/scoring.py - metrics integration)
- **GitHub Fetchers**: 0% (Unused module)
- **Legacy Metrics**: 0% (Deprecated code)

### Coverage Improvement Strategy ğŸ¯

1. **Flask App Testing**: Add tests for 18 REST API endpoints (775 lines untested)
2. **Integration Testing**: Test Flask + Phase 1 metrics integration
3. **Error Path Testing**: Add negative test cases for API endpoints
4. **Authentication Testing**: Test JWT tokens and permissions
5. **Database Testing**: Add tests for model/user persistence
6. **Performance Testing**: API response time validation

---

## ğŸ› ï¸ **Test Configuration**

### pytest Configuration (`pyproject.toml`)
```toml
[tool.pytest.ini_options]
addopts = "-q --cov=src --cov=app --cov-report=term-missing"
asyncio_mode = "auto"  # Automatic async test support
markers = [
    "unit: Unit tests",
    "integration: Integration tests", 
    "api: API tests",
    "backend: Backend tests",
    "slow: Slow-running tests",
    "security: Security-related tests"
]
```

### Test Dependencies
- **pytest**: Test framework
- **pytest-asyncio**: Async test support
- **pytest-cov**: Coverage reporting
- **unittest.mock**: Mocking framework

### Shared Fixtures (`conftest.py`)
- **Mock API Clients**: Git, GenAI, HuggingFace
- **Database Fixtures**: Mock database and S3
- **Authentication**: Mock tokens and sessions
- **Validation Helpers**: Data validation utilities

---

## ğŸš€ **Development Workflow**

### Running Tests During Development

```bash
# Quick feedback loop
python -m pytest tests/test_backend.py -v --no-cov

# Test specific functionality
python -m pytest -k "test_license" -v

# Watch mode (requires pytest-watch)
ptw tests/ src/

# Performance benchmarking
python -m pytest -m "slow" -v
```

### Test-Driven Development (TDD)

1. **Write failing test** for new feature
2. **Implement minimal code** to pass
3. **Refactor** while keeping tests green
4. **Add integration tests** for complete flows

### Continuous Integration

The test suite is designed for CI/CD with:
- **Fast feedback**: Core tests run in < 5 seconds
- **Parallel execution**: Tests can run concurrently
- **Clear reporting**: Detailed failure information
- **Coverage tracking**: Automatic coverage reports

---

## ğŸ¯ **Next Steps for Phase 2**

### Immediate Priorities
1. **Flask App Structure** - Implement the missing `app/` directory
2. **Database Schema** - Design models for registry, users, metrics
3. **API Implementation** - Build REST endpoints matching test scaffolding
4. **New Metrics** - Implement Reproducibility, Reviewedness, Treescore

### Test Integration Goals
- **Increase Coverage**: Target 60%+ overall coverage
- **Performance Validation**: Ensure API response times < 200ms
- **Security Testing**: Add authentication and authorization tests
- **Load Testing**: Validate concurrent user scenarios

---

## ğŸ“š **Additional Resources**

- **Phase 1 Specification**: Original CLI tool requirements
- **Phase 2 Specification**: REST API and registry requirements  
- **API Documentation**: OpenAPI/Swagger specs (coming soon)
- **Deployment Guide**: AWS infrastructure setup
- **Contributing Guide**: Development standards and practices

---

**Last Updated**: October 20, 2025  
**Test Suite Version**: Phase 2 Scaffolding v1.0  
**Maintainer**: ECE461 Team